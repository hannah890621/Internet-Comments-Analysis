{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvJvrX7i6y64",
        "outputId": "7c94ecea-a2d3-41ea-fa6d-c8a4e720443d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Tensorflow 1 is deprecated, and support will be removed on August 1, 2022.\n",
            "After that, `%tensorflow_version 1.x` will throw an error.\n",
            "\n",
            "Your notebook should be updated to use Tensorflow 2.\n",
            "See the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2.\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import logging\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "import re\n",
        "import jieba\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, cross_validate, RandomizedSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB  \n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.neighbors import KNeighborsClassifier  \n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier \n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn import metrics\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.python.keras import layers, models, optimizers\n",
        "from tensorflow.python.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from tensorflow.python.keras.preprocessing import text, sequence\n",
        "from tensorflow.python.keras.layers import Dense, Input, Flatten, Dropout, LSTM, BatchNormalization\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3l9HWJYbHem",
        "outputId": "d1361dcd-26bc-415c-f4b6-9a83791d7af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbbettp57ChK"
      },
      "outputs": [],
      "source": [
        "# 這邊記得替換成檔案的路徑\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/MosBurger/multi_label_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ekG8JKVz7KsR",
        "outputId": "88ffec9c-c824-4b1c-efbc-b358d495a49f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                            comment  \\\n",
              "0           0                             日出燒肉蛋堡中間是一顆炸過的水煮蛋味道還不錯   \n",
              "1           1  服務很親切，環境很舒適，餐點很美味，每個人都很殷勤工作，剛剛是“特殊兒”送餐來，很友善認真，...   \n",
              "2           2         店員服務態度佳，常常光顧，不知道為何會有人說店員態度不好，反而是很多客人態度差要求多   \n",
              "3           3                                 “換”敢不敢寫大一點把雞塊還我= =   \n",
              "4           4  2018-01 平日早上，點塔塔鱈魚堡，感覺size縮水了！服務友善，店內寬敞明亮，有很大的...   \n",
              "\n",
              "                     label  Service  Environment  Food  \n",
              "0                     Food        0            0     1  \n",
              "1  Service#Enviroment#Food        1            1     1  \n",
              "2                  Service        1            0     0  \n",
              "3                     Food        0            0     1  \n",
              "4          Food#Enviroment        0            1     1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75c95a81-1839-450d-a51b-02a4898ab0d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>comment</th>\n",
              "      <th>label</th>\n",
              "      <th>Service</th>\n",
              "      <th>Environment</th>\n",
              "      <th>Food</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>日出燒肉蛋堡中間是一顆炸過的水煮蛋味道還不錯</td>\n",
              "      <td>Food</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>服務很親切，環境很舒適，餐點很美味，每個人都很殷勤工作，剛剛是“特殊兒”送餐來，很友善認真，...</td>\n",
              "      <td>Service#Enviroment#Food</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>店員服務態度佳，常常光顧，不知道為何會有人說店員態度不好，反而是很多客人態度差要求多</td>\n",
              "      <td>Service</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>“換”敢不敢寫大一點把雞塊還我= =</td>\n",
              "      <td>Food</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-01 平日早上，點塔塔鱈魚堡，感覺size縮水了！服務友善，店內寬敞明亮，有很大的...</td>\n",
              "      <td>Food#Enviroment</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75c95a81-1839-450d-a51b-02a4898ab0d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-75c95a81-1839-450d-a51b-02a4898ab0d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-75c95a81-1839-450d-a51b-02a4898ab0d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "L9GMxOsWctDB",
        "outputId": "61f4457b-94e7-44b2-fa45-b9f9efc19417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 3.366 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                            comment  \\\n",
              "0           0                             日出燒肉蛋堡中間是一顆炸過的水煮蛋味道還不錯   \n",
              "1           1  服務很親切，環境很舒適，餐點很美味，每個人都很殷勤工作，剛剛是“特殊兒”送餐來，很友善認真，...   \n",
              "2           2         店員服務態度佳，常常光顧，不知道為何會有人說店員態度不好，反而是很多客人態度差要求多   \n",
              "3           3                                 “換”敢不敢寫大一點把雞塊還我= =   \n",
              "4           4  2018-01 平日早上，點塔塔鱈魚堡，感覺size縮水了！服務友善，店內寬敞明亮，有很大的...   \n",
              "\n",
              "                     label  Service  Environment  Food  \\\n",
              "0                     Food        0            0     1   \n",
              "1  Service#Enviroment#Food        1            1     1   \n",
              "2                  Service        1            0     0   \n",
              "3                     Food        0            0     1   \n",
              "4          Food#Enviroment        0            1     1   \n",
              "\n",
              "                                         comment_seg  \n",
              "0                  日出 燒 肉蛋 堡 中間 是 一顆 炸過 的 水煮蛋 味道 還不錯  \n",
              "1  服務 很 親切 ， 環境 很 舒適 ， 餐點 很 美味 ， 每個 人 都 很 殷勤 工作 ，...  \n",
              "2  店員 服務態度 佳 ， 常常 光顧 ， 不 知道 為 何會 有人 說 店員態度 不好 ， 反...  \n",
              "3                      “ 換 ” 敢不敢 寫大 一點 把 雞塊 還我 =   =  \n",
              "4  2018 - 01   平日 早上 ， 點塔塔 鱈 魚堡 ， 感覺 size 縮水 了 ！ ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c53e89ff-06bb-4808-9601-bce333c93610\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>comment</th>\n",
              "      <th>label</th>\n",
              "      <th>Service</th>\n",
              "      <th>Environment</th>\n",
              "      <th>Food</th>\n",
              "      <th>comment_seg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>日出燒肉蛋堡中間是一顆炸過的水煮蛋味道還不錯</td>\n",
              "      <td>Food</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>日出 燒 肉蛋 堡 中間 是 一顆 炸過 的 水煮蛋 味道 還不錯</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>服務很親切，環境很舒適，餐點很美味，每個人都很殷勤工作，剛剛是“特殊兒”送餐來，很友善認真，...</td>\n",
              "      <td>Service#Enviroment#Food</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>服務 很 親切 ， 環境 很 舒適 ， 餐點 很 美味 ， 每個 人 都 很 殷勤 工作 ，...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>店員服務態度佳，常常光顧，不知道為何會有人說店員態度不好，反而是很多客人態度差要求多</td>\n",
              "      <td>Service</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>店員 服務態度 佳 ， 常常 光顧 ， 不 知道 為 何會 有人 說 店員態度 不好 ， 反...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>“換”敢不敢寫大一點把雞塊還我= =</td>\n",
              "      <td>Food</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>“ 換 ” 敢不敢 寫大 一點 把 雞塊 還我 =   =</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-01 平日早上，點塔塔鱈魚堡，感覺size縮水了！服務友善，店內寬敞明亮，有很大的...</td>\n",
              "      <td>Food#Enviroment</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2018 - 01   平日 早上 ， 點塔塔 鱈 魚堡 ， 感覺 size 縮水 了 ！ ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c53e89ff-06bb-4808-9601-bce333c93610')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c53e89ff-06bb-4808-9601-bce333c93610 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c53e89ff-06bb-4808-9601-bce333c93610');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df['comment_seg'] = df['comment'].apply(lambda x: ' '.join(jieba.lcut(x)))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Caz-Q6_7N-L",
        "outputId": "36fc6d3b-6826-4822-969a-6880bc9adec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2871,) (718,) (2871,) (718,)\n"
          ]
        }
      ],
      "source": [
        "# 將資料中切出 20% 作為測試資料\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['comment_seg'], df['Environment'], test_size=0.2, random_state=42)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnXtfAyh7RdD",
        "outputId": "f97c6ba4-63d1-4c73-b519-737c1d5ed72d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2871x724 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 16227 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', min_df=10)\n",
        "count_vect.fit(df['comment_seg'])\n",
        "counts_train = CountVectorizer(vocabulary=count_vect.vocabulary_).fit_transform(X_train)\n",
        "counts_test = CountVectorizer(vocabulary=count_vect.vocabulary_).fit_transform(X_test)\n",
        "counts_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGb0mcTD7Uop",
        "outputId": "b6ece131-29a3-407f-bad9-a526eba7f600"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2871x724 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 16227 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', min_df=10)\n",
        "tfidf_vect.fit(df['comment_seg'])\n",
        "tfidf_train = TfidfVectorizer(vocabulary=tfidf_vect.vocabulary_).fit_transform(X_train)\n",
        "tfidf_test = TfidfVectorizer(vocabulary=tfidf_vect.vocabulary_).fit_transform(X_test)\n",
        "tfidf_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRTWXTXJ7XZz",
        "outputId": "3178e9aa-0fd8-4b9a-b9b8-3ae6c71f1fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-13 01:17:24,105 : INFO : running /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-4070a176-5551-49c5-9501-d798d217a044.json\n",
            "2022-07-13 01:17:24,151 : INFO : collecting all words and their counts\n",
            "2022-07-13 01:17:24,163 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2022-07-13 01:17:24,231 : INFO : collected 9238 word types from a corpus of 74369 raw words and 3589 sentences\n",
            "2022-07-13 01:17:24,238 : INFO : Loading a fresh vocabulary\n",
            "2022-07-13 01:17:24,271 : INFO : effective_min_count=1 retains 9238 unique words (100% of original 9238, drops 0)\n",
            "2022-07-13 01:17:24,278 : INFO : effective_min_count=1 leaves 74369 word corpus (100% of original 74369, drops 0)\n",
            "2022-07-13 01:17:24,328 : INFO : deleting the raw counts dictionary of 9238 items\n",
            "2022-07-13 01:17:24,336 : INFO : sample=0.001 downsamples 46 most-common words\n",
            "2022-07-13 01:17:24,342 : INFO : downsampling leaves estimated 53653 word corpus (72.1% of prior 74369)\n",
            "2022-07-13 01:17:24,372 : INFO : estimated required memory for 9238 words and 200 dimensions: 19399800 bytes\n",
            "2022-07-13 01:17:24,378 : INFO : resetting layer weights\n",
            "2022-07-13 01:17:28,632 : INFO : training model with 2 workers on 9238 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2022-07-13 01:17:28,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-07-13 01:17:28,824 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-07-13 01:17:28,828 : INFO : EPOCH - 1 : training on 74369 raw words (53705 effective words) took 0.2s, 303740 effective words/s\n",
            "2022-07-13 01:17:29,026 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-07-13 01:17:29,043 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-07-13 01:17:29,048 : INFO : EPOCH - 2 : training on 74369 raw words (53696 effective words) took 0.2s, 257963 effective words/s\n",
            "2022-07-13 01:17:29,271 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-07-13 01:17:29,275 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-07-13 01:17:29,276 : INFO : EPOCH - 3 : training on 74369 raw words (53588 effective words) took 0.2s, 248991 effective words/s\n",
            "2022-07-13 01:17:29,550 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-07-13 01:17:29,560 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-07-13 01:17:29,563 : INFO : EPOCH - 4 : training on 74369 raw words (53784 effective words) took 0.2s, 238623 effective words/s\n",
            "2022-07-13 01:17:29,833 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-07-13 01:17:29,845 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-07-13 01:17:29,851 : INFO : EPOCH - 5 : training on 74369 raw words (53637 effective words) took 0.2s, 216460 effective words/s\n",
            "2022-07-13 01:17:29,852 : INFO : training on a 371845 raw words (268410 effective words) took 1.2s, 221538 effective words/s\n"
          ]
        }
      ],
      "source": [
        "program = os.path.basename(sys.argv[0])\n",
        "logger = logging.getLogger(program)\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
        "w2v_model = Word2Vec(df['comment_seg'].apply(lambda x: x.split(' ',-1)), # input要是list不是str\n",
        "                     min_count=1,\n",
        "                     size=200,\n",
        "                     workers=multiprocessing.cpu_count())  # 訓練skip-gram模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYO2SOL_7eBX"
      },
      "outputs": [],
      "source": [
        "def AvgVector(w2v_model, sentence):\n",
        "  vec = []\n",
        "  for i in sentence.split(' ', -1):\n",
        "    if i in w2v_model.wv.index2word:\n",
        "      vec.append(w2v_model[i])\n",
        "  vector = np.mean(vec, axis=0)\n",
        "  vector = pd.Series(vector)\n",
        "  return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FWj4js-7rFm",
        "outputId": "6238a09c-6287-4d68-a51d-1cf0b4b6eb26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2871, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# %%time\n",
        "w2v_train = X_train.apply(lambda x: AvgVector(w2v_model, x))\n",
        "w2v_test = X_test.apply(lambda x: AvgVector(w2v_model, x))\n",
        "w2v_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFohljml7wd4"
      },
      "outputs": [],
      "source": [
        "# 也可直接讀取詞向量的模型即可\n",
        "# pretrain_w2v_model = Word2Vec.load(\"/content/drive/MyDrive/wordvec_wiki/wiki.zh.text.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSVeZeey8ER4",
        "outputId": "ebb96f66-fe79-4e39-8e1b-0664f8c93442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on Counts feature  ========================================\n",
            "Score on Train:  0.9212817833507488\n",
            "Score on Test:  0.8844011142061281\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = LogisticRegression(max_iter=1000, n_jobs=-1)   \n",
        "clf.fit(counts_train, y_train)\n",
        "print('='*40, ' Score on Counts feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(counts_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(counts_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBi6OAhC8Fe9",
        "outputId": "2e08e5a9-40ff-4ecd-cbde-7ae3d573c666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on TFIDF feature  ========================================\n",
            "Score on Train:  0.9056078021595263\n",
            "Score on Test:  0.871866295264624\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = LogisticRegression(max_iter=1000, n_jobs=-1)   \n",
        "clf.fit(tfidf_train, y_train)\n",
        "print('='*40, ' Score on TFIDF feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(tfidf_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(tfidf_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxOOhVWh8I4K",
        "outputId": "a439a49c-953a-45ba-dd88-0dee4cb12ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on AvgVector feature  ========================================\n",
            "Score on Train:  0.6283524904214559\n",
            "Score on Test:  0.6267409470752089\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = LogisticRegression(max_iter=1000, n_jobs=-1)   \n",
        "clf.fit(w2v_train, y_train)\n",
        "print('='*40, ' Score on AvgVector feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(w2v_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(w2v_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5dgvwZj8NDy",
        "outputId": "f2a21bd5-ac62-4c1e-fec3-5052b38f4517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on CV result  ========================================\n",
            "Best Score:  0.8599836388425997\n",
            "Best Params:  {'alpha': 1.4}\n",
            "========================================  Score on Counts feature  ========================================\n",
            "Score on Train:  0.8861024033437827\n",
            "Score on Test:  0.8649025069637883\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "param_grid = {'alpha':[1.4, 1.2, 1, 0.8, 0.6]}\n",
        "estimators = GridSearchCV(estimator = MultinomialNB(),\n",
        "                      param_grid = param_grid,\n",
        "                      n_jobs = -1,\n",
        "                      cv = 5)\n",
        "estimators.fit(counts_train, y_train)\n",
        "print('='*40, ' Score on CV result ', '='*40)\n",
        "print('Best Score: ', estimators.best_score_)\n",
        "print('Best Params: ', estimators.best_params_)\n",
        "clf = MultinomialNB(alpha = estimators.best_params_['alpha'])   \n",
        "clf.fit(counts_train, y_train)\n",
        "print('='*40, ' Score on Counts feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(counts_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(counts_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU0c11iK8Nly",
        "outputId": "92b8aafa-94f7-477d-bd8a-feeba78b2f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on CV result  ========================================\n",
            "Best Score:  0.8728725950613543\n",
            "Best Params:  {'alpha': 0.6}\n",
            "========================================  Score on TFIDF feature  ========================================\n",
            "Score on Train:  0.898989898989899\n",
            "Score on Test:  0.8649025069637883\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "param_grid = {'alpha':[1.4, 1.2, 1, 0.8, 0.6]}\n",
        "estimators = GridSearchCV(estimator = MultinomialNB(),\n",
        "                      param_grid = param_grid,\n",
        "                      n_jobs = -1,\n",
        "                      cv = 5)\n",
        "estimators.fit(tfidf_train, y_train)\n",
        "print('='*40, ' Score on CV result ', '='*40)\n",
        "print('Best Score: ', estimators.best_score_)\n",
        "print('Best Params: ', estimators.best_params_)\n",
        "# %%time\n",
        "clf = MultinomialNB(alpha = estimators.best_params_['alpha'])   \n",
        "clf.fit(tfidf_train, y_train)\n",
        "print('='*40, ' Score on TFIDF feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(tfidf_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(tfidf_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzpkA0ja8QRa",
        "outputId": "7bc3bc19-b689-423d-e667-f5c6e6828644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on CV result  ========================================\n",
            "Best Score:  0.5280424178154824\n",
            "Best Params:  {'alpha': 1.4}\n",
            "========================================  Score on AvgVector feature  ========================================\n",
            "Score on Train:  0.5245559038662487\n",
            "Score on Test:  0.5125348189415042\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(w2v_train)\n",
        "\n",
        "w2v_train_mm = scaler.transform(w2v_train)\n",
        "w2v_test_mm = scaler.transform(w2v_test)\n",
        "\n",
        "param_grid = {'alpha':[1.4, 1.2, 1, 0.8, 0.6]}\n",
        "estimators = GridSearchCV(estimator = MultinomialNB(),\n",
        "                        param_grid = param_grid,\n",
        "                        n_jobs = -1,\n",
        "                        cv = 5)\n",
        "estimators.fit(w2v_train_mm, y_train)\n",
        "print('='*40, ' Score on CV result ', '='*40)\n",
        "print('Best Score: ', estimators.best_score_)\n",
        "print('Best Params: ', estimators.best_params_)\n",
        "# %%time\n",
        "clf = MultinomialNB(alpha = estimators.best_params_['alpha'])   \n",
        "clf.fit(w2v_train_mm, y_train)\n",
        "print('='*40, ' Score on AvgVector feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(w2v_train_mm)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(w2v_test_mm)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcxiHCBg8S52",
        "outputId": "f261eea7-e735-4a36-c1bd-adb50eb29b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on Counts feature  ========================================\n",
            "Score on Train:  0.9331243469174504\n",
            "Score on Test:  0.8941504178272981\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(counts_train, y_train)\n",
        "print('='*40, ' Score on Counts feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(counts_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(counts_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVTJaM-Z8Wfz",
        "outputId": "d43f3287-c734-4181-95de-d37bf9486c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on TFIDF feature  ========================================\n",
            "Score on Train:  0.9233716475095786\n",
            "Score on Test:  0.8885793871866295\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(tfidf_train, y_train)\n",
        "print('='*40, ' Score on TFIDF feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(tfidf_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(tfidf_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72FGcoQa8YYO",
        "outputId": "4e6eaa1f-10d6-4840-dc4d-c5bcf1574581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on AvgVector feature  ========================================\n",
            "Score on Train:  0.5977011494252874\n",
            "Score on Test:  0.5947075208913649\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(w2v_train, y_train)\n",
        "print('='*40, ' Score on AvgVector feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(w2v_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(w2v_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pmtnrab8azs",
        "outputId": "6d945f82-b74f-45f7-909f-c71fff3873a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on CV result  ========================================\n",
            "Best Score:  0.8021639145584002\n",
            "Best Params:  {'n_neighbors': 3}\n",
            "========================================  Score on Counts feature  ========================================\n",
            "Score on Train:  0.8805294322535702\n",
            "Score on Test:  0.8064066852367688\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "param_grid = {'n_neighbors':list(range(1,9))}\n",
        "estimators = GridSearchCV(estimator = KNeighborsClassifier(),\n",
        "                        param_grid = param_grid,\n",
        "                        n_jobs = -1,\n",
        "                        cv = 5)\n",
        "estimators.fit(counts_train, y_train)\n",
        "print('='*40, ' Score on CV result ', '='*40)\n",
        "print('Best Score: ', estimators.best_score_)\n",
        "print('Best Params: ', estimators.best_params_)\n",
        "# %%time\n",
        "clf = KNeighborsClassifier(n_neighbors = estimators.best_params_['n_neighbors'])   \n",
        "clf.fit(counts_train, y_train)\n",
        "print('='*40, ' Score on Counts feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(counts_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(counts_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj2H4Fwu8dUS",
        "outputId": "03c789af-6e8e-48aa-d752-df1d66247e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on CV result  ========================================\n",
            "Best Score:  0.7662905620360551\n",
            "Best Params:  {'n_neighbors': 3}\n",
            "========================================  Score on TFIDF feature  ========================================\n",
            "Score on Train:  0.9097875304771856\n",
            "Score on Test:  0.7910863509749304\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "param_grid = {'n_neighbors':list(range(1,9))}\n",
        "estimators = GridSearchCV(estimator = KNeighborsClassifier(),\n",
        "                        param_grid = param_grid,\n",
        "                        n_jobs = -1,\n",
        "                        cv = 5)\n",
        "estimators.fit(tfidf_train, y_train)\n",
        "print('='*40, ' Score on CV result ', '='*40)\n",
        "print('Best Score: ', estimators.best_score_)\n",
        "print('Best Params: ', estimators.best_params_)\n",
        "# %%time\n",
        "clf = KNeighborsClassifier(n_neighbors = estimators.best_params_['n_neighbors'], n_jobs=-1)   \n",
        "clf.fit(tfidf_train, y_train)\n",
        "print('='*40, ' Score on TFIDF feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(tfidf_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(tfidf_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRviqput8fxw",
        "outputId": "adaca1e8-6c8d-4de3-80a5-9f1fe6179b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on CV result  ========================================\n",
            "Best Score:  0.622428116951977\n",
            "Best Params:  {'n_neighbors': 6}\n",
            "========================================  Score on AvgVector feature  ========================================\n",
            "Score on Train:  0.7290142807384187\n",
            "Score on Test:  0.6086350974930362\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "param_grid = {'n_neighbors':list(range(1,9))}\n",
        "estimators = GridSearchCV(estimator = KNeighborsClassifier(),\n",
        "                        param_grid = param_grid,\n",
        "                        n_jobs = -1,\n",
        "                        cv = 5)\n",
        "estimators.fit(w2v_train, y_train)\n",
        "print('='*40, ' Score on CV result ', '='*40)\n",
        "print('Best Score: ', estimators.best_score_)\n",
        "print('Best Params: ', estimators.best_params_)\n",
        "# %%time\n",
        "clf = KNeighborsClassifier(n_neighbors = estimators.best_params_['n_neighbors'], n_jobs=-1)   \n",
        "clf.fit(w2v_train, y_train)\n",
        "print('='*40, ' Score on AvgVector feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(w2v_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(w2v_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1buqlrx8h9X",
        "outputId": "27678125-a2b0-455b-e270-f15eae4003b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on Counts feature  ========================================\n",
            "Score on Train:  0.9780564263322884\n",
            "Score on Test:  0.8871866295264624\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = RandomForestClassifier(n_estimators = 500, max_features = 'sqrt', n_jobs=-1, random_state = 10)  \n",
        "clf.fit(counts_train, y_train)\n",
        "print('='*40, ' Score on Counts feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(counts_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(counts_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxwcikV98j5i",
        "outputId": "e072dbe2-3546-4bd0-f1c9-8e51f8426cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on TFIDF feature  ========================================\n",
            "Score on Train:  0.9780564263322884\n",
            "Score on Test:  0.8969359331476323\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = RandomForestClassifier(n_estimators = 500, max_features = 'sqrt', n_jobs=-1, random_state = 10)  \n",
        "clf.fit(tfidf_train, y_train)\n",
        "print('='*40, ' Score on TFIDF feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(tfidf_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(tfidf_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA1F2RGN8l5X",
        "outputId": "dad73f13-0004-470c-cb16-cf84d34a9af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on AvgVector feature  ========================================\n",
            "Score on Train:  0.9972135144548938\n",
            "Score on Test:  0.658774373259053\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = RandomForestClassifier(n_estimators = 500, max_features = 'sqrt', n_jobs=-1, random_state = 10)  \n",
        "clf.fit(w2v_train, y_train)\n",
        "print('='*40, ' Score on AvgVector feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(w2v_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(w2v_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfkN32QA8oPu",
        "outputId": "435b40ce-b369-460c-d631-e01e018b6221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on Counts feature  ========================================\n",
            "Score on Train:  0.9261581330546848\n",
            "Score on Test:  0.8871866295264624\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = xgb.XGBClassifier(n_estimators=500, objective='multi:softmax', n_jobs=-1, silent=False, num_class=2)\n",
        "clf.fit(counts_train, y_train)\n",
        "print('='*40, ' Score on Counts feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(counts_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(counts_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhXugw-f8qV3",
        "outputId": "6faa7a72-97e8-4740-d519-c2e3c69c5dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on TFIDF feature  ========================================\n",
            "Score on Train:  0.9373040752351097\n",
            "Score on Test:  0.8913649025069638\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = xgb.XGBClassifier(n_estimators=500, objective='multi:softmax', n_jobs=-1, silent=False, num_class=2)\n",
        "clf.fit(tfidf_train, y_train)\n",
        "print('='*40, ' Score on TFIDF feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(tfidf_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(tfidf_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5qAJgrQ8sWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e2d617-b9d9-4449-d951-2b0fc950122a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================  Score on AvgVector feature  ========================================\n",
            "Score on Train:  0.9752699407871822\n",
            "Score on Test:  0.6963788300835655\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "clf = xgb.XGBClassifier(n_estimators=500, objective='multi:softmax', n_jobs=-1, silent=False, num_class=2)\n",
        "clf.fit(w2v_train, y_train)\n",
        "print('='*40, ' Score on AvgVector feature ', '='*40)\n",
        "print('Score on Train: ', metrics.accuracy_score(y_train, clf.predict(w2v_train)))\n",
        "print('Score on Test: ', metrics.accuracy_score(y_test, clf.predict(w2v_test)))\n",
        "print('='*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7isIDBm68ytr",
        "outputId": "2883061a-4716-422d-9232-f204f9fecc31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9101 unique tokens.\n",
            "Shape of counts_train tensor: (2871, 10000)\n",
            "Shape of counts_test tensor: (718, 10000)\n",
            "Shape of y_train_dummy tensor: (2871, 2)\n",
            "Shape of y_test_dummy tensor: (718, 2)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-13 01:18:46,524 : WARNING : From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               5120512   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 5,288,066\n",
            "Trainable params: 5,286,530\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n",
            "Train on 2296 samples, validate on 575 samples\n",
            "Epoch 1/100\n",
            "2296/2296 [==============================] - 1s 599us/sample - loss: 0.4542 - acc: 0.7953 - val_loss: 0.6979 - val_acc: 0.3983\n",
            "Epoch 2/100\n",
            "2296/2296 [==============================] - 0s 91us/sample - loss: 0.0650 - acc: 0.9826 - val_loss: 0.6973 - val_acc: 0.3983\n",
            "Epoch 3/100\n",
            "2296/2296 [==============================] - 0s 92us/sample - loss: 0.0225 - acc: 0.9943 - val_loss: 0.7073 - val_acc: 0.3983\n",
            "Epoch 4/100\n",
            "2296/2296 [==============================] - 0s 96us/sample - loss: 0.0169 - acc: 0.9952 - val_loss: 0.7026 - val_acc: 0.3983\n",
            "Epoch 5/100\n",
            "2296/2296 [==============================] - 0s 101us/sample - loss: 0.0184 - acc: 0.9952 - val_loss: 0.7090 - val_acc: 0.3983\n",
            "Epoch 6/100\n",
            "2296/2296 [==============================] - 0s 98us/sample - loss: 0.0112 - acc: 0.9939 - val_loss: 0.7096 - val_acc: 0.3983\n",
            "Epoch 7/100\n",
            "2296/2296 [==============================] - 0s 98us/sample - loss: 0.0119 - acc: 0.9943 - val_loss: 0.7149 - val_acc: 0.3983\n",
            "Epoch 8/100\n",
            "2296/2296 [==============================] - 0s 96us/sample - loss: 0.0133 - acc: 0.9943 - val_loss: 0.7109 - val_acc: 0.4000\n",
            "Epoch 9/100\n",
            "2296/2296 [==============================] - 0s 102us/sample - loss: 0.0129 - acc: 0.9943 - val_loss: 0.7022 - val_acc: 0.4035\n",
            "Epoch 10/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0117 - acc: 0.9948 - val_loss: 0.7046 - val_acc: 0.4052\n",
            "Epoch 11/100\n",
            "2296/2296 [==============================] - 0s 92us/sample - loss: 0.0104 - acc: 0.9948 - val_loss: 0.7072 - val_acc: 0.4070\n",
            "Epoch 12/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0109 - acc: 0.9939 - val_loss: 0.7002 - val_acc: 0.4122\n",
            "Epoch 13/100\n",
            "2296/2296 [==============================] - 0s 91us/sample - loss: 0.0109 - acc: 0.9930 - val_loss: 0.6950 - val_acc: 0.4243\n",
            "Epoch 14/100\n",
            "2296/2296 [==============================] - 0s 97us/sample - loss: 0.0121 - acc: 0.9952 - val_loss: 0.6913 - val_acc: 0.4626\n",
            "Epoch 15/100\n",
            "2296/2296 [==============================] - 0s 95us/sample - loss: 0.0106 - acc: 0.9943 - val_loss: 0.6866 - val_acc: 0.4609\n",
            "Epoch 16/100\n",
            "2296/2296 [==============================] - 0s 92us/sample - loss: 0.0105 - acc: 0.9961 - val_loss: 0.6525 - val_acc: 0.5287\n",
            "Epoch 17/100\n",
            "2296/2296 [==============================] - 0s 101us/sample - loss: 0.0102 - acc: 0.9943 - val_loss: 0.6577 - val_acc: 0.5287\n",
            "Epoch 18/100\n",
            "2296/2296 [==============================] - 0s 96us/sample - loss: 0.0107 - acc: 0.9961 - val_loss: 0.6376 - val_acc: 0.5757\n",
            "Epoch 19/100\n",
            "2296/2296 [==============================] - 0s 94us/sample - loss: 0.0098 - acc: 0.9939 - val_loss: 0.6134 - val_acc: 0.6000\n",
            "Epoch 20/100\n",
            "2296/2296 [==============================] - 0s 90us/sample - loss: 0.0093 - acc: 0.9956 - val_loss: 0.6041 - val_acc: 0.6296\n",
            "Epoch 21/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0087 - acc: 0.9956 - val_loss: 0.5372 - val_acc: 0.6991\n",
            "Epoch 22/100\n",
            "2296/2296 [==============================] - 0s 95us/sample - loss: 0.0087 - acc: 0.9952 - val_loss: 0.5397 - val_acc: 0.7165\n",
            "Epoch 23/100\n",
            "2296/2296 [==============================] - 0s 95us/sample - loss: 0.0091 - acc: 0.9952 - val_loss: 0.5145 - val_acc: 0.7391\n",
            "Epoch 24/100\n",
            "2296/2296 [==============================] - 0s 87us/sample - loss: 0.0088 - acc: 0.9948 - val_loss: 0.5103 - val_acc: 0.7478\n",
            "Epoch 25/100\n",
            "2296/2296 [==============================] - 0s 89us/sample - loss: 0.0090 - acc: 0.9961 - val_loss: 0.4693 - val_acc: 0.7774\n",
            "Epoch 26/100\n",
            "2296/2296 [==============================] - 0s 89us/sample - loss: 0.0092 - acc: 0.9956 - val_loss: 0.4683 - val_acc: 0.7826\n",
            "Epoch 27/100\n",
            "2296/2296 [==============================] - 0s 95us/sample - loss: 0.0088 - acc: 0.9948 - val_loss: 0.4382 - val_acc: 0.8035\n",
            "Epoch 28/100\n",
            "2296/2296 [==============================] - 0s 91us/sample - loss: 0.0102 - acc: 0.9943 - val_loss: 0.4153 - val_acc: 0.8243\n",
            "Epoch 29/100\n",
            "2296/2296 [==============================] - 0s 87us/sample - loss: 0.0096 - acc: 0.9956 - val_loss: 0.4097 - val_acc: 0.8278\n",
            "Epoch 30/100\n",
            "2296/2296 [==============================] - 0s 90us/sample - loss: 0.0108 - acc: 0.9948 - val_loss: 0.3817 - val_acc: 0.8400\n",
            "Epoch 31/100\n",
            "2296/2296 [==============================] - 0s 92us/sample - loss: 0.0091 - acc: 0.9935 - val_loss: 0.3622 - val_acc: 0.8487\n",
            "Epoch 32/100\n",
            "2296/2296 [==============================] - 0s 97us/sample - loss: 0.0094 - acc: 0.9961 - val_loss: 0.3533 - val_acc: 0.8626\n",
            "Epoch 33/100\n",
            "2296/2296 [==============================] - 0s 96us/sample - loss: 0.0090 - acc: 0.9948 - val_loss: 0.3559 - val_acc: 0.8591\n",
            "Epoch 34/100\n",
            "2296/2296 [==============================] - 0s 100us/sample - loss: 0.0081 - acc: 0.9956 - val_loss: 0.3830 - val_acc: 0.8504\n",
            "Epoch 35/100\n",
            "2296/2296 [==============================] - 0s 89us/sample - loss: 0.0083 - acc: 0.9943 - val_loss: 0.3729 - val_acc: 0.8643\n",
            "Epoch 36/100\n",
            "2296/2296 [==============================] - 0s 97us/sample - loss: 0.0081 - acc: 0.9961 - val_loss: 0.3660 - val_acc: 0.8626\n",
            "Epoch 37/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0083 - acc: 0.9952 - val_loss: 0.3600 - val_acc: 0.8643\n",
            "Epoch 38/100\n",
            "2296/2296 [==============================] - 0s 85us/sample - loss: 0.0079 - acc: 0.9943 - val_loss: 0.3763 - val_acc: 0.8626\n",
            "Epoch 39/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0074 - acc: 0.9956 - val_loss: 0.3874 - val_acc: 0.8678\n",
            "Epoch 40/100\n",
            "2296/2296 [==============================] - 0s 89us/sample - loss: 0.0076 - acc: 0.9952 - val_loss: 0.4003 - val_acc: 0.8678\n",
            "Epoch 41/100\n",
            "2296/2296 [==============================] - 0s 97us/sample - loss: 0.0075 - acc: 0.9948 - val_loss: 0.4011 - val_acc: 0.8678\n",
            "Epoch 42/100\n",
            "2296/2296 [==============================] - 0s 96us/sample - loss: 0.0078 - acc: 0.9952 - val_loss: 0.4099 - val_acc: 0.8713\n",
            "Epoch 43/100\n",
            "2296/2296 [==============================] - 0s 147us/sample - loss: 0.0078 - acc: 0.9956 - val_loss: 0.4197 - val_acc: 0.8730\n",
            "Epoch 44/100\n",
            "2296/2296 [==============================] - 0s 185us/sample - loss: 0.0077 - acc: 0.9952 - val_loss: 0.4549 - val_acc: 0.8626\n",
            "Epoch 45/100\n",
            "2296/2296 [==============================] - 0s 186us/sample - loss: 0.0088 - acc: 0.9961 - val_loss: 0.4533 - val_acc: 0.8678\n",
            "Epoch 46/100\n",
            "2296/2296 [==============================] - 0s 164us/sample - loss: 0.0081 - acc: 0.9948 - val_loss: 0.4517 - val_acc: 0.8730\n",
            "Epoch 47/100\n",
            "2296/2296 [==============================] - 0s 105us/sample - loss: 0.0075 - acc: 0.9948 - val_loss: 0.4699 - val_acc: 0.8626\n",
            "Epoch 48/100\n",
            "2296/2296 [==============================] - 0s 100us/sample - loss: 0.0077 - acc: 0.9956 - val_loss: 0.4672 - val_acc: 0.8661\n",
            "Epoch 49/100\n",
            "2296/2296 [==============================] - 0s 90us/sample - loss: 0.0097 - acc: 0.9935 - val_loss: 0.4514 - val_acc: 0.8765\n",
            "Epoch 50/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0072 - acc: 0.9952 - val_loss: 0.4572 - val_acc: 0.8748\n",
            "Epoch 51/100\n",
            "2296/2296 [==============================] - 0s 92us/sample - loss: 0.0085 - acc: 0.9961 - val_loss: 0.4556 - val_acc: 0.8765\n",
            "Epoch 52/100\n",
            "2296/2296 [==============================] - 0s 86us/sample - loss: 0.0074 - acc: 0.9956 - val_loss: 0.4637 - val_acc: 0.8748\n",
            "Epoch 53/100\n",
            "2296/2296 [==============================] - 0s 83us/sample - loss: 0.0074 - acc: 0.9961 - val_loss: 0.4704 - val_acc: 0.8748\n",
            "Epoch 54/100\n",
            "2296/2296 [==============================] - 0s 92us/sample - loss: 0.0075 - acc: 0.9961 - val_loss: 0.4738 - val_acc: 0.8748\n",
            "Epoch 55/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0074 - acc: 0.9956 - val_loss: 0.4814 - val_acc: 0.8765\n",
            "Epoch 56/100\n",
            "2296/2296 [==============================] - 0s 98us/sample - loss: 0.0075 - acc: 0.9952 - val_loss: 0.4841 - val_acc: 0.8765\n",
            "Epoch 57/100\n",
            "2296/2296 [==============================] - 0s 85us/sample - loss: 0.0077 - acc: 0.9952 - val_loss: 0.4826 - val_acc: 0.8765\n",
            "Epoch 58/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0072 - acc: 0.9943 - val_loss: 0.4767 - val_acc: 0.8817\n",
            "Epoch 59/100\n",
            "2296/2296 [==============================] - 0s 86us/sample - loss: 0.0073 - acc: 0.9952 - val_loss: 0.4753 - val_acc: 0.8800\n",
            "Epoch 60/100\n",
            "2296/2296 [==============================] - 0s 95us/sample - loss: 0.0071 - acc: 0.9952 - val_loss: 0.4793 - val_acc: 0.8748\n",
            "Epoch 61/100\n",
            "2296/2296 [==============================] - 0s 98us/sample - loss: 0.0075 - acc: 0.9943 - val_loss: 0.4855 - val_acc: 0.8748\n",
            "Epoch 62/100\n",
            "2296/2296 [==============================] - 0s 90us/sample - loss: 0.0072 - acc: 0.9952 - val_loss: 0.4919 - val_acc: 0.8748\n",
            "Epoch 63/100\n",
            "2296/2296 [==============================] - 0s 87us/sample - loss: 0.0071 - acc: 0.9952 - val_loss: 0.4963 - val_acc: 0.8748\n",
            "Epoch 64/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0070 - acc: 0.9961 - val_loss: 0.4964 - val_acc: 0.8748\n",
            "Epoch 65/100\n",
            "2296/2296 [==============================] - 0s 87us/sample - loss: 0.0073 - acc: 0.9956 - val_loss: 0.5004 - val_acc: 0.8730\n",
            "Epoch 66/100\n",
            "2296/2296 [==============================] - 0s 96us/sample - loss: 0.0074 - acc: 0.9956 - val_loss: 0.5058 - val_acc: 0.8730\n",
            "Epoch 67/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0071 - acc: 0.9952 - val_loss: 0.5073 - val_acc: 0.8748\n",
            "Epoch 68/100\n",
            "2296/2296 [==============================] - 0s 88us/sample - loss: 0.0072 - acc: 0.9956 - val_loss: 0.5042 - val_acc: 0.8748\n",
            "Epoch 00068: early stopping\n",
            "718/718 [==============================] - 0s 103us/sample - loss: 0.7008 - acc: 0.8496\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.70082599082365, 0.8495822]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "## 設定超參數\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "MOMENTUM = 0.95\n",
        "# 建立資料格式\n",
        "# 考量時間與記憶體容量，僅保留數量最多的1萬個詞\n",
        "tokenizer = text.Tokenizer(num_words=10000) \n",
        "tokenizer.fit_on_texts(df['comment_seg'])\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "counts_train = tokenizer.texts_to_sequences(X_train) \n",
        "counts_train = tokenizer.sequences_to_matrix(counts_train, mode='freq')\n",
        "counts_test = tokenizer.texts_to_sequences(X_test)\n",
        "counts_test = tokenizer.sequences_to_matrix(counts_test, mode='freq')\n",
        "print('Shape of counts_train tensor:', counts_train.shape)\n",
        "print('Shape of counts_test tensor:', counts_test.shape)\n",
        "\n",
        "y_train_dummy = to_categorical(np.asarray(y_train))\n",
        "y_test_dummy = to_categorical(np.asarray(y_test))\n",
        "print('Shape of y_train_dummy tensor:', y_train_dummy.shape)\n",
        "print('Shape of y_test_dummy tensor:', y_test_dummy.shape)\n",
        "# 搭建模型框架\n",
        "keras.backend.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Dense(units=512, input_shape=(counts_train.shape[1],), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y_train_dummy.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "# 載入 Callbacks, 並將 monitor 設定為監控 validation loss\n",
        "earlystop = EarlyStopping(monitor=\"val_acc\", \n",
        "                        patience=10, \n",
        "                        verbose=1)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=optimizers.Adam(lr=LEARNING_RATE, epsilon=None, decay=0.0),\n",
        "            metrics=['accuracy'])\n",
        "# %%time\n",
        "model.fit(counts_train, y_train_dummy,\n",
        "        epochs=EPOCHS, \n",
        "        validation_split=0.2,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        callbacks=[earlystop])\n",
        "model.evaluate(counts_test, y_test_dummy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0FMwu2E81xa",
        "outputId": "f271e8e7-77ff-4e0b-85ac-28298d1203be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9101 unique tokens.\n",
            "Shape of tfidf_train tensor: (2871, 10000)\n",
            "Shape of tfidf_test tensor: (718, 10000)\n",
            "Shape of y_train tensor: (2871, 2)\n",
            "Shape of y_test tensor: (718, 2)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               5120512   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 5,288,066\n",
            "Trainable params: 5,286,530\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n",
            "Train on 2296 samples, validate on 575 samples\n",
            "Epoch 1/100\n",
            "2296/2296 [==============================] - 1s 227us/sample - loss: 0.5638 - acc: 0.7452 - val_loss: 0.5877 - val_acc: 0.8296\n",
            "Epoch 2/100\n",
            "2296/2296 [==============================] - 0s 98us/sample - loss: 0.0824 - acc: 0.9782 - val_loss: 0.5323 - val_acc: 0.8522\n",
            "Epoch 3/100\n",
            "2296/2296 [==============================] - 0s 97us/sample - loss: 0.0352 - acc: 0.9909 - val_loss: 0.4576 - val_acc: 0.8591\n",
            "Epoch 4/100\n",
            "2296/2296 [==============================] - 0s 96us/sample - loss: 0.0247 - acc: 0.9917 - val_loss: 0.4063 - val_acc: 0.8713\n",
            "Epoch 5/100\n",
            "2296/2296 [==============================] - 0s 94us/sample - loss: 0.0181 - acc: 0.9943 - val_loss: 0.3578 - val_acc: 0.8783\n",
            "Epoch 6/100\n",
            "2296/2296 [==============================] - 0s 99us/sample - loss: 0.0171 - acc: 0.9935 - val_loss: 0.3236 - val_acc: 0.8817\n",
            "Epoch 7/100\n",
            "2296/2296 [==============================] - 0s 105us/sample - loss: 0.0187 - acc: 0.9948 - val_loss: 0.3035 - val_acc: 0.8870\n",
            "Epoch 8/100\n",
            "2296/2296 [==============================] - 0s 95us/sample - loss: 0.0158 - acc: 0.9943 - val_loss: 0.2937 - val_acc: 0.8835\n",
            "Epoch 9/100\n",
            "2296/2296 [==============================] - 0s 95us/sample - loss: 0.0128 - acc: 0.9948 - val_loss: 0.2902 - val_acc: 0.8800\n",
            "Epoch 10/100\n",
            "2296/2296 [==============================] - 0s 98us/sample - loss: 0.0138 - acc: 0.9943 - val_loss: 0.2668 - val_acc: 0.8904\n",
            "Epoch 11/100\n",
            "2296/2296 [==============================] - 0s 97us/sample - loss: 0.0114 - acc: 0.9935 - val_loss: 0.2639 - val_acc: 0.8939\n",
            "Epoch 12/100\n",
            "2296/2296 [==============================] - 0s 99us/sample - loss: 0.0109 - acc: 0.9956 - val_loss: 0.2621 - val_acc: 0.8939\n",
            "Epoch 13/100\n",
            "2296/2296 [==============================] - 0s 96us/sample - loss: 0.0135 - acc: 0.9930 - val_loss: 0.2649 - val_acc: 0.8922\n",
            "Epoch 14/100\n",
            "2296/2296 [==============================] - 0s 95us/sample - loss: 0.0124 - acc: 0.9952 - val_loss: 0.2643 - val_acc: 0.8922\n",
            "Epoch 15/100\n",
            "2296/2296 [==============================] - 0s 102us/sample - loss: 0.0110 - acc: 0.9948 - val_loss: 0.2624 - val_acc: 0.8974\n",
            "Epoch 16/100\n",
            "2296/2296 [==============================] - 0s 108us/sample - loss: 0.0114 - acc: 0.9935 - val_loss: 0.2635 - val_acc: 0.8974\n",
            "Epoch 17/100\n",
            "2296/2296 [==============================] - 0s 105us/sample - loss: 0.0119 - acc: 0.9952 - val_loss: 0.2679 - val_acc: 0.8974\n",
            "Epoch 18/100\n",
            "2296/2296 [==============================] - 0s 97us/sample - loss: 0.0117 - acc: 0.9952 - val_loss: 0.2694 - val_acc: 0.9026\n",
            "Epoch 19/100\n",
            "2296/2296 [==============================] - 0s 97us/sample - loss: 0.0124 - acc: 0.9943 - val_loss: 0.2763 - val_acc: 0.9009\n",
            "Epoch 20/100\n",
            "2296/2296 [==============================] - 0s 100us/sample - loss: 0.0134 - acc: 0.9939 - val_loss: 0.2801 - val_acc: 0.9061\n",
            "Epoch 21/100\n",
            "2296/2296 [==============================] - 0s 100us/sample - loss: 0.0118 - acc: 0.9952 - val_loss: 0.2818 - val_acc: 0.9009\n",
            "Epoch 22/100\n",
            "2296/2296 [==============================] - 0s 95us/sample - loss: 0.0123 - acc: 0.9961 - val_loss: 0.2916 - val_acc: 0.9009\n",
            "Epoch 23/100\n",
            "2296/2296 [==============================] - 0s 97us/sample - loss: 0.0103 - acc: 0.9961 - val_loss: 0.2986 - val_acc: 0.8974\n",
            "Epoch 24/100\n",
            "2296/2296 [==============================] - 0s 98us/sample - loss: 0.0076 - acc: 0.9961 - val_loss: 0.3045 - val_acc: 0.8991\n",
            "Epoch 25/100\n",
            "2296/2296 [==============================] - 0s 101us/sample - loss: 0.0118 - acc: 0.9965 - val_loss: 0.3092 - val_acc: 0.8991\n",
            "Epoch 00025: early stopping\n",
            "718/718 [==============================] - 0s 98us/sample - loss: 0.4547 - acc: 0.8565\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.45472575935812715, 0.856546]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "## 超參數設定\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "MOMENTUM = 0.95\n",
        "# 建立資料格式\n",
        "# 考量時間與記憶體容量，僅保留數量最多的1萬個詞\n",
        "tokenizer = text.Tokenizer(num_words=10000) \n",
        "tokenizer.fit_on_texts(df['comment_seg'])\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "tfidf_train = tokenizer.texts_to_sequences(X_train) \n",
        "tfidf_train = tokenizer.sequences_to_matrix(tfidf_train, mode='tfidf')\n",
        "tfidf_test = tokenizer.texts_to_sequences(X_test)\n",
        "tfidf_test = tokenizer.sequences_to_matrix(tfidf_test, mode='tfidf')\n",
        "print('Shape of tfidf_train tensor:', tfidf_train.shape)\n",
        "print('Shape of tfidf_test tensor:', tfidf_test.shape)\n",
        "\n",
        "y_train_dummy = to_categorical(np.asarray(y_train))\n",
        "y_test_dummy = to_categorical(np.asarray(y_test))\n",
        "print('Shape of y_train tensor:', y_train_dummy.shape)\n",
        "print('Shape of y_test tensor:', y_test_dummy.shape)\n",
        "# 搭建模型框架\n",
        "keras.backend.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Dense(units=512, input_shape=(tfidf_train.shape[1],), activation='relu')) ################\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y_train_dummy.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "# 載入 Callbacks, 並將 monitor 設定為監控 validation loss\n",
        "earlystop = EarlyStopping(monitor=\"val_acc\", \n",
        "                        patience=5, \n",
        "                        verbose=1)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=optimizers.Adam(lr=LEARNING_RATE, epsilon=None, decay=0.0),\n",
        "            metrics=['accuracy'])\n",
        "# %%time\n",
        "model.fit(tfidf_train, y_train_dummy,\n",
        "        epochs=EPOCHS, \n",
        "        validation_split=0.2,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        callbacks=[earlystop])\n",
        "model.evaluate(tfidf_test, y_test_dummy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsTr12YH844D",
        "outputId": "5bc733f0-42f8-47e3-e7a1-9bbbfd6d4a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9101 unique tokens.\n",
            "Shape of vec_train tensor: (2871, 300)\n",
            "Shape of vec_test tensor: (718, 300)\n",
            "Shape of y_train_dummy tensor: (2871, 2)\n",
            "Shape of y_test_dummy tensor: (718, 2)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "2022-07-13 01:19:16,728 : WARNING : From /tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 200)          1820400   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 300, 200)          800       \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 298, 256)          153856    \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 99, 256)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25344)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               5069000   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 402       \n",
            "=================================================================\n",
            "Total params: 7,044,458\n",
            "Trainable params: 5,223,658\n",
            "Non-trainable params: 1,820,800\n",
            "_________________________________________________________________\n",
            "Train on 2296 samples, validate on 575 samples\n",
            "Epoch 1/100\n",
            "2296/2296 [==============================] - 3s 1ms/sample - loss: 2.2695 - acc: 0.5065 - val_loss: 0.6925 - val_acc: 0.6000\n",
            "Epoch 2/100\n",
            "2296/2296 [==============================] - 0s 132us/sample - loss: 0.6921 - acc: 0.5854 - val_loss: 0.6917 - val_acc: 0.6017\n",
            "Epoch 3/100\n",
            "2296/2296 [==============================] - 0s 131us/sample - loss: 0.6912 - acc: 0.5858 - val_loss: 0.6899 - val_acc: 0.6017\n",
            "Epoch 4/100\n",
            "2296/2296 [==============================] - 0s 127us/sample - loss: 0.6898 - acc: 0.5858 - val_loss: 0.6880 - val_acc: 0.6017\n",
            "Epoch 5/100\n",
            "2296/2296 [==============================] - 0s 130us/sample - loss: 0.6882 - acc: 0.5858 - val_loss: 0.6863 - val_acc: 0.6017\n",
            "Epoch 6/100\n",
            "2296/2296 [==============================] - 0s 130us/sample - loss: 0.6868 - acc: 0.5858 - val_loss: 0.6847 - val_acc: 0.6017\n",
            "Epoch 7/100\n",
            "2296/2296 [==============================] - 0s 130us/sample - loss: 0.6856 - acc: 0.5858 - val_loss: 0.6832 - val_acc: 0.6017\n",
            "Epoch 00007: early stopping\n",
            "718/718 [==============================] - 0s 109us/sample - loss: 0.6849 - acc: 0.5864\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6849386929469521, 0.586351]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "MAX_SEQUENCE_LENGTH = 300 # 每条新闻最大长度\n",
        "EMBEDDING_DIM = 200 # 词向量空间维度\n",
        "\n",
        "## 超參數設定\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "MOMENTUM = 0.95\n",
        "# 建立資料格式\n",
        "tokenizer = text.Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(df['comment_seg'])\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "vec_train = tokenizer.texts_to_sequences(X_train) \n",
        "vec_train = sequence.pad_sequences(vec_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "vec_test = tokenizer.texts_to_sequences(X_test)\n",
        "vec_test = sequence.pad_sequences(vec_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of vec_train tensor:', vec_train.shape)\n",
        "print('Shape of vec_test tensor:', vec_test.shape)\n",
        "\n",
        "y_train_dummy = to_categorical(np.asarray(y_train))\n",
        "y_test_dummy = to_categorical(np.asarray(y_test))\n",
        "print('Shape of y_train_dummy tensor:', y_train_dummy.shape)\n",
        "print('Shape of y_test_dummy tensor:', y_test_dummy.shape)\n",
        "# 將詞替換成對應的向量\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items(): \n",
        "    if word in w2v_model:\n",
        "        embedding_matrix[i] = np.asarray(w2v_model[word],\n",
        "                                        dtype='float32')\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "# 搭建模型框架\n",
        "keras.backend.clear_session()\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(256, 3, padding='valid', activation='relu', strides=1))\n",
        "model.add(MaxPooling1D(3))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
        "model.add(Dense(y_train_dummy.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "# 載入 Callbacks, 並將 monitor 設定為監控 validation loss\n",
        "earlystop = EarlyStopping(monitor=\"val_acc\", \n",
        "                        patience=5, \n",
        "                        verbose=1)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer= optimizers.Adam(lr=LEARNING_RATE, epsilon=None, decay=0.0),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "model.fit(vec_train, y_train_dummy,\n",
        "        epochs=EPOCHS, \n",
        "        validation_split=0.2,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        callbacks=[earlystop])\n",
        "model.evaluate(vec_test, y_test_dummy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4Y1U-nR88AZ",
        "outputId": "f7b620e2-52bf-4e0a-c028-88e88e5b8584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9101 unique tokens.\n",
            "Shape of vec_train tensor: (2871, 300)\n",
            "Shape of vec_test tensor: (718, 300)\n",
            "Shape of y_train tensor: (2871, 2)\n",
            "Shape of y_test tensor: (718, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 200)          1820400   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 300, 200)          800       \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 298, 256)          153856    \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 99, 256)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25344)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               5069000   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 402       \n",
            "=================================================================\n",
            "Total params: 7,044,458\n",
            "Trainable params: 5,223,658\n",
            "Non-trainable params: 1,820,800\n",
            "_________________________________________________________________\n",
            "Train on 2296 samples, validate on 575 samples\n",
            "Epoch 1/100\n",
            "2296/2296 [==============================] - 0s 207us/sample - loss: 1.9531 - acc: 0.5531 - val_loss: 0.6853 - val_acc: 0.6017\n",
            "Epoch 2/100\n",
            "2296/2296 [==============================] - 0s 126us/sample - loss: 0.6710 - acc: 0.5858 - val_loss: 0.6816 - val_acc: 0.6017\n",
            "Epoch 3/100\n",
            "2296/2296 [==============================] - 0s 131us/sample - loss: 0.6610 - acc: 0.5858 - val_loss: 0.6819 - val_acc: 0.6017\n",
            "Epoch 4/100\n",
            "2296/2296 [==============================] - 0s 129us/sample - loss: 0.6624 - acc: 0.5858 - val_loss: 0.6804 - val_acc: 0.6017\n",
            "Epoch 5/100\n",
            "2296/2296 [==============================] - 0s 127us/sample - loss: 0.6569 - acc: 0.5858 - val_loss: 0.6806 - val_acc: 0.6017\n",
            "Epoch 6/100\n",
            "2296/2296 [==============================] - 0s 133us/sample - loss: 0.6531 - acc: 0.5858 - val_loss: 0.6843 - val_acc: 0.6017\n",
            "Epoch 7/100\n",
            "2296/2296 [==============================] - 0s 132us/sample - loss: 0.6507 - acc: 0.5858 - val_loss: 0.6822 - val_acc: 0.6017\n",
            "Epoch 8/100\n",
            "2296/2296 [==============================] - 0s 132us/sample - loss: 0.6469 - acc: 0.5858 - val_loss: 0.6821 - val_acc: 0.6017\n",
            "Epoch 9/100\n",
            "2296/2296 [==============================] - 0s 131us/sample - loss: 0.6480 - acc: 0.5858 - val_loss: 0.6817 - val_acc: 0.6017\n",
            "Epoch 00009: early stopping\n",
            "718/718 [==============================] - 0s 99us/sample - loss: 0.6822 - acc: 0.5864\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6822423312325332, 0.586351]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "MAX_SEQUENCE_LENGTH = 300 # 每条新闻最大长度\n",
        "EMBEDDING_DIM = 200 # 词向量空间维度\n",
        "\n",
        "## 超參數設定\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "MOMENTUM = 0.95\n",
        "\n",
        "# 建立資料格式\n",
        "tokenizer = text.Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(df['comment_seg'])\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "vec_train = tokenizer.texts_to_sequences(X_train) \n",
        "vec_train = sequence.pad_sequences(vec_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "vec_test = tokenizer.texts_to_sequences(X_test)\n",
        "vec_test = sequence.pad_sequences(vec_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of vec_train tensor:', vec_train.shape)\n",
        "print('Shape of vec_test tensor:', vec_test.shape)\n",
        "\n",
        "y_train_dummy = to_categorical(np.asarray(y_train))\n",
        "y_test_dummy = to_categorical(np.asarray(y_test))\n",
        "print('Shape of y_train tensor:', y_train_dummy.shape)\n",
        "print('Shape of y_test tensor:', y_test_dummy.shape)\n",
        "# 將詞替換成對應的向量\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items(): \n",
        "    if word in w2v_model:\n",
        "        embedding_matrix[i] = np.asarray(w2v_model[word],\n",
        "                                         dtype='float32')\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "# 搭建模型框架\n",
        "keras.backend.clear_session()\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(256, 3, padding='valid', activation='relu', strides=1))\n",
        "model.add(MaxPooling1D(3))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
        "model.add(Dense(y_train_dummy.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "# 載入 Callbacks, 並將 monitor 設定為監控 validation loss\n",
        "earlystop = EarlyStopping(monitor=\"val_loss\", \n",
        "                        patience=5, \n",
        "                        verbose=1)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer= optimizers.Adam(lr=LEARNING_RATE, epsilon=None, decay=0.0),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "model.fit(vec_train, y_train_dummy,\n",
        "        epochs=EPOCHS, \n",
        "        validation_split=0.2,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        callbacks=[earlystop])\n",
        "model.evaluate(vec_test, y_test_dummy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "28JW7v4YEet4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}